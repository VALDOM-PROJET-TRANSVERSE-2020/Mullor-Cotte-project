{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "notebook_tf_transform.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpmc-39De3Fm"
      },
      "source": [
        "# WHY FAKE NEWS IS A PROBLEM?\n",
        "**Fake news refers to misinformation, disinformation or mal-information which is spread through word of mouth and traditional media and more recently through digital forms of communication such as edited videos, memes, unverified advertisements and social media propagated rumours.Fake news spread through social media has become a serious problem, with the potential of it resulting in mob violence, suicides etc as a result of misinformation circulated on social media.**\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TanaiRJxe3Fq"
      },
      "source": [
        "# BRIEF DESCRIPTION OF DATASET\n",
        "**This dataset consists of about 40000 articles consisting of fake as well as real news. Our aim is train our model so that it can correctly predict whether a given piece of news is real or fake.The fake and real news data is given in two separate datasets with each dataset consisting around 20000 articles each.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkyuGUx_e3Fs"
      },
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWSJpsyKqHjH",
        "outputId": "78efe49f-a548-40a3-8430-2dcf783af364"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLQM2IdHfV5q",
        "outputId": "df56b416-70a8-48e9-85ab-f004b6cf6148"
      },
      "source": [
        "!ls\n",
        "%cd /content/drive/My Drive/Colab Notebooks/projet_sopra/input/fake-and-real-news-dataset"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  sample_data\n",
            "/content/drive/My Drive/Colab Notebooks/projet_sopra/input/fake-and-real-news-dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtFsswgHfvfG",
        "outputId": "bb7191e7-15ba-4ce0-ef96-6787f9b6863c"
      },
      "source": [
        "%cd fake-and-real-news-dataset/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'fake-and-real-news-dataset/'\n",
            "/content/drive/My Drive/Colab Notebooks/projet_sopra/input/fake-and-real-news-dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKA79zTZe3F6"
      },
      "source": [
        "# LOADING THE NECESSARY LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCdX3tI3e3F8"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from bs4 import BeautifulSoup\n",
        "import re,string,unicodedata\n",
        "from keras.preprocessing import text, sequence\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from string import punctuation\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Embedding,LSTM,Dropout\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "import tensorflow as tf"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWqyQbXWe3GF"
      },
      "source": [
        "# IMPORTING THE DATASET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6rp6eHXe3GI"
      },
      "source": [
        "# true = pd.read_csv(\"../input/fake-and-real-news-dataset/True.csv\")\n",
        "# false = pd.read_csv(\"../input/fake-and-real-news-dataset/Fake.csv\")\n",
        "true = pd.read_csv(\"True.csv\")\n",
        "false = pd.read_csv(\"Fake.csv\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIMQql4Ee3GR"
      },
      "source": [
        "# DATA VISUALIZATION AND PREPROCESSING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrITcogj7mxi"
      },
      "source": [
        "**Import data in tf dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Am-njqT-NPTF"
      },
      "source": [
        "true_dataset = tf.data.experimental.CsvDataset(filenames = ['True.csv'], record_defaults = [tf.string, tf.string], select_cols=[0,1], header=True)\r\n",
        "false_dataset = tf.data.experimental.CsvDataset(filenames = ['Fake.csv'], record_defaults = [tf.string, tf.string], select_cols=[0,1], header=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEfVKurc7uva"
      },
      "source": [
        "**Join text and title**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9epNz60lzli"
      },
      "source": [
        "def preprocess_join_sentence(s1, s2):\r\n",
        "    ret = tf.strings.reduce_join([s1, s2],separator=\" \", axis=-1)\r\n",
        "    return ret\r\n",
        "\r\n",
        "true_dataset = true_dataset.map(preprocess_join_sentence)\r\n",
        "false_dataset = false_dataset.map(preprocess_join_sentence)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jvw_0r_W2mh2"
      },
      "source": [
        "### Add labels to dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_lo08LlZczU"
      },
      "source": [
        "cat_true = tf.data.Dataset.from_tensor_slices(np.ones(len(list(true_dataset)), dtype=int))\r\n",
        "cat_false = tf.data.Dataset.from_tensor_slices(np.zeros(len(list(false_dataset)), dtype=int))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fj09KwCJZjdu"
      },
      "source": [
        "true_dataset = tf.data.Dataset.zip((true_dataset, cat_true))\r\n",
        "false_dataset = tf.data.Dataset.zip((false_dataset, cat_false))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggohx0giaB8c"
      },
      "source": [
        "full_dataset = false_dataset.concatenate(true_dataset)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2eM0q3174EE"
      },
      "source": [
        "**Formate sentences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ttqepq_OhNe3"
      },
      "source": [
        "# def preprocess_sentence(sentence, cat):\r\n",
        "def preprocess_sentence(sentence, label):\r\n",
        "  ret = tf.strings.lower(sentence)\r\n",
        "  # ret = BeautifulSoup(ret, \"html.parser\").get_text()\r\n",
        "  ret = tf.strings.strip(ret)\r\n",
        "  ret = tf.strings.regex_replace(ret, '<[^<]+?>', '') # Removing HTML chars\r\n",
        "  ret = tf.strings.regex_replace(ret, '\\[[^]]*\\]', '') # Removing the square brackets\r\n",
        "  ret = tf.strings.regex_replace(ret, 'http\\S+', '') # Removing URL's\r\n",
        "  return ret, label\r\n",
        "\r\n",
        "full_dataset = full_dataset.map(preprocess_sentence)"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9Q44zZzkZ16",
        "outputId": "c6c952b4-408a-4c6b-db51-abe30547b560"
      },
      "source": [
        "for idx, row in enumerate(full_dataset):\r\n",
        "    print (row)\r\n",
        "    if idx > 3:\r\n",
        "        break"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'donald trump sends out embarrassing new year\\xe2\\x80\\x99s eve message; this is disturbing donald trump just couldn t wish all americans a happy new year and leave it at that. instead, he had to give a shout out to his enemies, haters and  the very dishonest fake news media.  the former reality show star had just one job to do and he couldn t do it. as our country rapidly grows stronger and smarter, i want to wish all of my friends, supporters, enemies, haters, and even the very dishonest fake news media, a happy and healthy new year,  president angry pants tweeted.  2018 will be a great year for america! as our country rapidly grows stronger and smarter, i want to wish all of my friends, supporters, enemies, haters, and even the very dishonest fake news media, a happy and healthy new year. 2018 will be a great year for america!  donald j. trump (@realdonaldtrump) december 31, 2017trump s tweet went down about as welll as you d expect.what kind of president sends a new year s greeting like this despicable, petty, infantile gibberish? only trump! his lack of decency won t even allow him to rise above the gutter long enough to wish the american citizens a happy new year!  bishop talbert swan (@talbertswan) december 31, 2017no one likes you  calvin (@calvinstowell) december 31, 2017your impeachment would make 2018 a great year for america, but i ll also accept regaining control of congress.  miranda yaver (@mirandayaver) december 31, 2017do you hear yourself talk? when you have to include that many people that hate you you have to wonder? why do the they all hate me?  alan sandoval (@alansandoval13) december 31, 2017who uses the word haters in a new years wish??  marlene (@marlene399) december 31, 2017you can t just say happy new year?  koren pollitt (@korencarpenter) december 31, 2017here s trump s new year s eve tweet from 2016.happy new year to all, including to my many enemies and those who have fought me and lost so badly they just don t know what to do. love!  donald j. trump (@realdonaldtrump) december 31, 2016this is nothing new for trump. he s been doing this for years.trump has directed messages to his  enemies  and  haters  for new year s, easter, thanksgiving, and the anniversary of 9/11. pic.twitter.com/4fpae2kypa  daniel dale (@ddale8) december 31, 2017trump s holiday tweets are clearly not presidential.how long did he work at hallmark before becoming president?  steven goodine (@sgoodine) december 31, 2017he s always been like this . . . the only difference is that in the last few years, his filter has been breaking down.  roy schulze (@thbthttt) december 31, 2017who, apart from a teenager uses the term haters?  wendy (@wendywhistles) december 31, 2017he s a fucking 5 year old  who knows (@rainyday80) december 31, 2017so, to all the people who voted for this a hole thinking he would change once he got into power, you were wrong! 70-year-old men don t change and now he s a year older.photo by andrew burton/getty images.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'drunk bragging trump staffer started russian collusion investigation house intelligence committee chairman devin nunes is going to have a bad day. he s been under the assumption, like many of us, that the christopher steele-dossier was what prompted the russia investigation so he s been lashing out at the department of justice and the fbi in order to protect trump. as it happens, the dossier is not what started the investigation, according to documents obtained by the new york times.former trump campaign adviser george papadopoulos was drunk in a wine bar when he revealed knowledge of russian opposition research on hillary clinton.on top of that, papadopoulos wasn t just a covfefe boy for trump, as his administration has alleged. he had a much larger role, but none so damning as being a drunken fool in a wine bar. coffee boys  don t help to arrange a new york meeting between trump and president abdel fattah el-sisi of egypt two months before the election. it was known before that the former aide set up meetings with world leaders for trump, but team trump ran with him being merely a coffee boy.in may 2016, papadopoulos revealed to australian diplomat alexander downer that russian officials were shopping around possible dirt on then-democratic presidential nominee hillary clinton. exactly how much mr. papadopoulos said that night at the kensington wine rooms with the australian, alexander downer, is unclear,  the report states.  but two months later, when leaked democratic emails began appearing online, australian officials passed the information about mr. papadopoulos to their american counterparts, according to four current and former american and foreign officials with direct knowledge of the australians  role. papadopoulos pleaded guilty to lying to the f.b.i. and is now a cooperating witness with special counsel robert mueller s team.this isn t a presidency. it s a badly scripted reality tv show.photo by win mcnamee/getty images.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'sheriff david clarke becomes an internet joke for threatening to poke people \\xe2\\x80\\x98in the eye\\xe2\\x80\\x99 on friday, it was revealed that former milwaukee sheriff david clarke, who was being considered for homeland security secretary in donald trump s administration, has an email scandal of his own.in january, there was a brief run-in on a plane between clarke and fellow passenger dan black, who he later had detained by the police for no reason whatsoever, except that maybe his feelings were hurt. clarke messaged the police to stop black after he deplaned, and now, a search warrant has been executed by the fbi to see the exchanges.clarke is calling it fake news even though copies of the search warrant are on the internet. i am unintimidated by lib media attempts to smear and discredit me with their fake news reports designed to silence me,  the former sheriff tweeted.  i will continue to poke them in the eye with a sharp stick and bitch slap these scum bags til they get it. i have been attacked by better people than them #maga i am unintimidated by lib media attempts to smear and discredit me with their fake news reports designed to silence me. i will continue to poke them in the eye with a sharp stick and bitch slap these scum bags til they get it. i have been attacked by better people than them #maga pic.twitter.com/xtzw5pdu2b  david a. clarke, jr. (@sheriffclarke) december 30, 2017he didn t stop there.breaking news! when lying lib media makes up fake news to smear me, the antidote is go right at them. punch them in the nose & make them taste their own blood. nothing gets a bully like lying lib media s attention better than to give them a taste of their own blood #neverbackdown pic.twitter.com/t2ny2pshcr  david a. clarke, jr. (@sheriffclarke) december 30, 2017the internet called him out.this is your local newspaper and that search warrant isn t fake, and just because the chose not to file charges at the time doesn t mean they won t! especially if you continue to lie. months after decision not to charge clarke, email search warrant filed   keithleblanc (@keithleblanc63) december 30, 2017i just hope the rest of the village people aren t implicated.  kirk ketchum (@kirkketchum) december 30, 2017slaw, baked potatoes, or french fries? pic.twitter.com/fwfxszupxy  alt- immigration   (@alt_uscis) december 30, 2017pic.twitter.com/ymsobljfxu  pendulum swinger (@pendulumswngr) december 30, 2017you called your police friends to stand up for you when someone made fun of your hat  chris jackson (@chriscjackson) december 30, 2017is it me, with this masterful pshop of your hat, which i seem to never tire of. i think it s the steely resolve in your one visible eye pic.twitter.com/dwr5k8zezv  chris mohney (@chrismohney) december 30, 2017are you indicating with your fingers how many people died in your jail? i think you re a few fingers short, dipshit  ike barinholtz (@ikebarinholtz) december 30, 2017rofl. internet tough guy with fake flair. pic.twitter.com/ulcfddhkdy  kellmecrazy (@kel_moonface) december 30, 2017you re so edgy, buddy.  mrs. smh (@mrssmh2) december 30, 2017is his break over at applebees?  aaron (@feltrrr2) december 30, 2017are you trying to earn your  still relevant  badge?  circusrebel (@circusdrew) december 30, 2017make sure to hydrate, drink lots of water. it s rumored that prisoners can be denied water by prison officials.  robert klinc (@robertklinc1) december 30, 2017terrill thomas, the 38-year-old black man who died of thirst in clarke s milwaukee county jail cell this april, was a victim of homicide. we just thought we should point that out. it can t be repeated enough.photo by spencer platt/getty images.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'trump is so obsessed he even has obama\\xe2\\x80\\x99s name coded into his website (images) on christmas day, donald trump announced that he would  be back to work  the following day, but he is golfing for the fourth day in a row. the former reality show star blasted former president barack obama for playing golf and now trump is on track to outpace the number of golf games his predecessor played.updated my tracker of trump s appearances at trump properties.71 rounds of golf including today s. at this pace, he ll pass obama s first-term total by july 24 next year.  pic.twitter.com/5gemcjqtbh  philip bump (@pbump) december 29, 2017 that makes what a washington post reporter discovered on trump s website really weird, but everything about this administration is bizarre af. the coding contained a reference to obama and golf:  unlike obama, we are working to fix the problem   and not on the golf course.  however, the coding wasn t done correctly.the website of donald trump, who has spent several days in a row at the golf course, is coded to serve up the following message in the event of an internal server error:  pic.twitter.com/wiqsqnnzw0  christopher ingraham (@_cingraham) december 28, 2017that snippet of code appears to be on all  pages, which the footer says is paid for by the rnc? pic.twitter.com/oazdt126b3  christopher ingraham (@_cingraham) december 28, 2017it s also all over  as others have noted in this thread, this is weird code and it s not clear it would ever actually display, but who knows.  christopher ingraham (@_cingraham) december 28, 2017after the coding was called out, the reference to obama was deleted.update: the golf error message has been removed from the trump and gop websites. they also fixed the javascript  =  vs  ==  problem. still not clear when these messages would actually display, since the actual 404 (and presumably 500) page displays a different message pic.twitter.com/z7dmyq5smy  christopher ingraham (@_cingraham) december 29, 2017that suggests someone at either rnc or the trump admin is sensitive enough to trump s golf problem to make this issue go away quickly once people noticed. you have no idea how much i d love to see the email exchange that led us here.  christopher ingraham (@_cingraham) december 29, 2017 the code was f-cked up.the best part about this is that they are using the  =  (assignment) operator which means that bit of code will never get run. if you look a few lines up  errorcode  will always be  404          (@tw1trsux) december 28, 2017trump s coders can t code. nobody is surprised.  tim peterson (@timrpeterson) december 28, 2017donald trump is obsessed with obama that his name was even in the coding of his website while he played golf again.photo by joe raedle/getty images.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'pope francis just called out donald trump during his christmas speech pope francis used his annual christmas day message to rebuke donald trump without even mentioning his name. the pope delivered his message just days after members of the united nations condemned trump s move to recognize jerusalem as the capital of israel. the pontiff prayed on monday for the  peaceful coexistence of two states within mutually agreed and internationally recognized borders. we see jesus in the children of the middle east who continue to suffer because of growing tensions between israelis and palestinians,  francis said.  on this festive day, let us ask the lord for peace for jerusalem and for all the holy land. let us pray that the will to resume dialogue may prevail between the parties and that a negotiated solution can finally be reached. the pope went on to plead for acceptance of refugees who have been forced from their homes, and that is an issue trump continues to fight against. francis used jesus for which there was  no place in the inn  as an analogy. today, as the winds of war are blowing in our world and an outdated model of development continues to produce human, societal and environmental decline, christmas invites us to focus on the sign of the child and to recognize him in the faces of little children, especially those for whom, like jesus,  there is no place in the inn,  he said. jesus knows well the pain of not being welcomed and how hard it is not to have a place to lay one s head,  he added.  may our hearts not be closed as they were in the homes of bethlehem. the pope said that mary and joseph were immigrants who struggled to find a safe place to stay in bethlehem. they had to leave their people, their home, and their land,  francis said.  this was no comfortable or easy journey for a young couple about to have a child.   at heart, they were full of hope and expectation because of the child about to be born; yet their steps were weighed down by the uncertainties and dangers that attend those who have to leave their home behind. so many other footsteps are hidden in the footsteps of joseph and mary,  francis said sunday. we see the tracks of entire families forced to set out in our own day. we see the tracks of millions of persons who do not choose to go away, but driven from their land, leave behind their dear ones. amen to that.photo by christopher furlong/getty images.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3hIj-EFXTgA"
      },
      "source": [
        "!pip install -q tensorflow-text\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmYi8yhdsLpM"
      },
      "source": [
        "### Glove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxSCs1TKmnmw",
        "outputId": "bb15a46a-d746-4dbc-db53-6ccaa629d43d"
      },
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\r\n",
        "# Build vocabulary and relevent vector\r\n",
        "words = []\r\n",
        "# Adding 2 empty vector of dimension 100 which will be used in padding and unknown token(word)\r\n",
        "# According to previous knowledge i have idea that it contains 400,000 words\r\n",
        "# Added extra 2 rows because when we will use inside TextVectorization add blank i.e. '' and ['UNK'] inside vocabulary\r\n",
        "vectors = np.zeros((400002, 50))\r\n",
        "\r\n",
        "# with open(f\"../glove-twitter/glove.twitter.27B.100d.txt\") as f:\r\n",
        "with open(f\"../glove-twitter/glove.6B.50d.txt\") as f:\r\n",
        "    lines = f.readlines()\r\n",
        "    for idx, line in enumerate(lines):\r\n",
        "        split_line = line.split()\r\n",
        "        words.append(split_line[0])\r\n",
        "        vectors[idx+2] = split_line[1:]\r\n",
        "\r\n",
        "print(f\"Number of words are {len(words)}\")\r\n",
        "print(f\"Shape of vector is {vectors.shape}\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words are 400000\n",
            "Shape of vector is (400002, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4XMjb7SsKCt",
        "outputId": "8e4eec93-992c-4aeb-fcc0-ad6071626bef"
      },
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\r\n",
        "# TextVectorization hyper parameter\r\n",
        "max_vocab = 400002\r\n",
        "max_len = 300\r\n",
        "vectorize_layer_glove = TextVectorization(max_tokens=max_vocab,\r\n",
        "                                                    standardize=\"lower_and_strip_punctuation\",\r\n",
        "                                                    split=\"whitespace\",\r\n",
        "                                                    output_mode=\"int\",\r\n",
        "                                                    output_sequence_length=max_len)\r\n",
        "\r\n",
        "\r\n",
        "# Vocabulary set into layer\r\n",
        "vectorize_layer_glove.set_vocabulary(words)\r\n",
        "print(f\"Top 10 words {vectorize_layer_glove.get_vocabulary()[:10]}\")\r\n",
        "print(f\"Some words {vectorize_layer_glove.get_vocabulary()[-10010:-10000]} \\n\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top 10 words ['', '[UNK]', 'the', ',', '.', 'of', 'to', 'and', 'in', 'a']\n",
            "Some words ['czermin', 'basketbal', 'marcheschi', '10-cents', 'stafon', 'haripal', 'rahmane', 'valdivielso', 'schleifstein', 'pyongyong'] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w77OJDFFsUDN"
      },
      "source": [
        "# vectorize_layer_glove.adapt(full_dataset)\r\n",
        "def int_vectorize_text(text, label):\r\n",
        "  text = tf.expand_dims(text, -1)\r\n",
        "  # return vectorize_layer_glove(text)\r\n",
        "  return vectorize_layer_glove(text), label\r\n",
        "\r\n",
        "train_data_tokenized = full_dataset.map(int_vectorize_text)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZhEbW1qsYC-"
      },
      "source": [
        "for idx, row in enumerate(train_data_tokenized):\r\n",
        "    print (row)\r\n",
        "    if idx > 3:\r\n",
        "        break\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDFp_AiVng2R",
        "outputId": "d381b3b8-a3e7-4d5d-804a-12d3ab3b3dcc"
      },
      "source": [
        "len(vectors[0])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qN27WAForl3F"
      },
      "source": [
        "### Prepare data to train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZXYqxoVro8W"
      },
      "source": [
        "# Model params\r\n",
        "batch_size = 256\r\n",
        "epochs = 10\r\n",
        "embed_size = 50\r\n",
        "maxlen = 300\r\n",
        "max_features = 400002\r\n",
        "\r\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvWBijprsD2Q"
      },
      "source": [
        "**Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8O1u2Wj8sDHn"
      },
      "source": [
        "#Defining Neural Network\r\n",
        "model = Sequential()\r\n",
        "#Non-trainable embeddidng layer\r\n",
        "# model_glove.add(tf.keras.layers.Embedding(max_vocab, 50, weights=[vectors], trainable=False))\r\n",
        "model.add(Embedding(max_features, output_dim=embed_size, weights=[vectors], input_length=maxlen, trainable=False))\r\n",
        "#LSTM \r\n",
        "model.add(LSTM(units=128 , return_sequences = True , recurrent_dropout = 0.25 , dropout = 0.25))\r\n",
        "model.add(LSTM(units=64 , recurrent_dropout = 0.1 , dropout = 0.1))\r\n",
        "model.add(Dense(units = 32 , activation = 'relu'))\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "model.compile(optimizer=keras.optimizers.Adam(lr = 0.01), loss='binary_crossentropy', metrics=['accuracy'])\r\n",
        "\r\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.5, min_lr=0.00001)\r\n"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGfBpf3uubNM",
        "outputId": "e6908f57-3371-49fb-e5c6-1edfd9e34ef9"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_7 (Embedding)      (None, 300, 50)           20000100  \n",
            "_________________________________________________________________\n",
            "lstm_10 (LSTM)               (None, 300, 128)          91648     \n",
            "_________________________________________________________________\n",
            "lstm_11 (LSTM)               (None, 64)                49408     \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 20,143,269\n",
            "Trainable params: 143,169\n",
            "Non-trainable params: 20,000,100\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxEGtL-85NSD"
      },
      "source": [
        "### TODO : add validation dataset and shuffle data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OitBqgQI0QEh"
      },
      "source": [
        "# dataset = train_data_tokenized.batch(batch_size).repeat()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Vo1h89w1sJK"
      },
      "source": [
        "train_data = train_data_tokenized.map(lambda x_text, x_label: (x_text, tf.expand_dims(x_label, -1)))"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cJWKSxH1ypE"
      },
      "source": [
        "for idx, row in enumerate(train_data):\r\n",
        "    print (row)\r\n",
        "    if idx > 3:\r\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBsJNoEjr4Ma",
        "outputId": "6b824f51-b007-45b6-8f88-d3224b121d15"
      },
      "source": [
        "# Trains for 5 epochs\r\n",
        "model.fit(train_data, epochs=5, steps_per_epoch=100)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "100/100 [==============================] - 81s 762ms/step - loss: 0.0626 - accuracy: 0.9485\n",
            "Epoch 2/5\n",
            "100/100 [==============================] - 79s 787ms/step - loss: 6.9772e-08 - accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "100/100 [==============================] - 77s 766ms/step - loss: 6.9235e-08 - accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "100/100 [==============================] - 77s 766ms/step - loss: 6.8215e-08 - accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "100/100 [==============================] - 76s 764ms/step - loss: 6.7374e-08 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f6680b431d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0liR2oO2CWt"
      },
      "source": [
        "# old code -> to copy\r\n",
        "# history = model.fit(x_train, y_train, batch_size = batch_size , validation_data = (X_test,y_test) , epochs = epochs , callbacks = [learning_rate_reduction])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpAEAyR_5rCO"
      },
      "source": [
        "# OLD notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcJz9UD855wC"
      },
      "source": [
        "# OLD notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JFZO0EG5qkW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tI2702EKmIe5"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.corpus import stopwords\r\n",
        "import tensorflow_text as text\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def remove_stopwords(sentence):\r\n",
        "  final_text = []\r\n",
        "  tokenizer = text.WhitespaceTokenizer()\r\n",
        "  tokens = tokenizer.tokenize([sentence.encode('UTF-8')])\r\n",
        "  unigram = text.ngrams(tokens, 1, reduction_type=text.Reduction.STRING_JOIN)\r\n",
        "\r\n",
        "  for i in unigram:\r\n",
        "    print(i)\r\n",
        "    if i not in stop:\r\n",
        "        final_text.append(unigram)\r\n",
        "  return \" \".join(final_text)\r\n",
        "  # return preprocess_sentence(sentence1), preprocess_sentence(sentence2)\r\n",
        "\r\n",
        "def drop_stop_words (text):\r\n",
        "    stop = set(stopwords.words('english'))\r\n",
        "    nltk.download('punkt')\r\n",
        "    punctuation = list(string.punctuation)\r\n",
        "    stop.update(punctuation)\r\n",
        "    words = word_tokenize(text)\r\n",
        "    a = ''\r\n",
        "    for index, txt in enumerate(words):\r\n",
        "        # a = ' '.join([w for w in words if w not in stop_words ] )\r\n",
        "        a = tf.strings.reduce_join([w for w in words if w not in stop_words ],separator=\" \", axis=-1)\r\n",
        "    return a\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_awxjWRp0f2"
      },
      "source": [
        "sentence = \"She is a very famous girl named Jane\"\r\n",
        "# remove_stopwords(sentence, stop)\r\n",
        "my_preprocessed_dataset = joined_dataset.map(remove_stopwords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oM80JjeE-2l"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop = set(stopwords.words('english'))\n",
        "punctuation = list(string.punctuation)\n",
        "stop.update(punctuation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOM6QCGbYzVj"
      },
      "source": [
        "from nltk.corpus import stopwords\r\n",
        "import tensorflow_text as text\r\n",
        "\r\n",
        "tokenizer = text.WhitespaceTokenizer()\r\n",
        "tokenized_docs = joined_dataset.map(lambda x: tokenizer.tokenize(x))\r\n",
        "iterator = iter(tokenized_docs)\r\n",
        "print(next(iterator))\r\n",
        "print(next(iterator))\r\n",
        "\r\n",
        "stop_words = set(stopwords.words('english'))\r\n",
        "for index, txt in enumerate(tokenized_docs):\r\n",
        "  print(txt)\r\n",
        "  # a = tf.strings.reduce_join([w for w in tokenized_docs if w not in stop_words ],separator=\" \", axis=-1)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKPn0cg9E_z8"
      },
      "source": [
        "def strip_html(text):\r\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\r\n",
        "    return soup.get_text()\r\n",
        "\r\n",
        "#Removing the square brackets\r\n",
        "def remove_between_square_brackets(text):\r\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\r\n",
        "# Removing URL's\r\n",
        "def remove_between_square_brackets(text):\r\n",
        "    return re.sub(r'http\\S+', '', text)\r\n",
        "#Removing the stopwords from text\r\n",
        "def remove_stopwords(text):\r\n",
        "    final_text = []\r\n",
        "    for i in text.split():\r\n",
        "        if i.strip().lower() not in stop:\r\n",
        "            final_text.append(i.strip())\r\n",
        "    return \" \".join(final_text)\r\n",
        "#Removing the noisy text\r\n",
        "def denoise_text(text):\r\n",
        "    text = strip_html(text)\r\n",
        "    text = remove_between_square_brackets(text)\r\n",
        "    text = remove_stopwords(text)\r\n",
        "    return text\r\n",
        "#Apply function on review column\r\n",
        "df['text']=df['text'].apply(denoise_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBcurIC5pbzx"
      },
      "source": [
        "my_preprocessed_dataset = true_dataset.map(preprocess)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "De-x9rUmp8Lx"
      },
      "source": [
        "for idx, row in enumerate(my_preprocessed_dataset):\r\n",
        "    print (row[0])\r\n",
        "    if idx > 3:\r\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UcWLy82qCGx"
      },
      "source": [
        "def preprocess_join(sentence1, sentence2):\r\n",
        "  def preprocess_join_sentence(s1, s2):\r\n",
        "    ret = tf.strings.reduce_join([s1, s2],separator=\" \", axis=-1)\r\n",
        "    return ret\r\n",
        "  return preprocess_join_sentence(sentence1, sentence2)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjlOPDhWrxzs"
      },
      "source": [
        "joined_dataset = true_dataset.map(preprocess_join)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ig20glWLgMPO"
      },
      "source": [
        "for idx, row in enumerate(joined_dataset):\r\n",
        "    print (row)\r\n",
        "    if idx > 3:\r\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzywHmikVdgk"
      },
      "source": [
        "joined_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMSEpz_KO0zC"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ly5HH_QZe3GS"
      },
      "source": [
        "true.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9yPaoyae3Ga"
      },
      "source": [
        "false.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-M3SXUfVe3Gl"
      },
      "source": [
        "true['category'] = 1\n",
        "false['category'] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4drEiI1e3Gu"
      },
      "source": [
        "df = pd.concat([true,false]) #Merging the 2 datasets\r\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGIhrtl4e3Gz"
      },
      "source": [
        "sns.set_style(\"darkgrid\")\n",
        "sns.countplot(df.category)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFPYg98ge3G5"
      },
      "source": [
        "**SO, WE CAN SEE THAT THE DATASET IS BALANCED**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwwIVtude3G6"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AB5m2i6ARo4Z"
      },
      "source": [
        "pd.set_option('display.max_colwidth', -1)\r\n",
        "df[-3:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeTOCfD3e3G_"
      },
      "source": [
        "df.isna().sum() # Checking for nan Values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4p4zoYje3HE"
      },
      "source": [
        "df.title.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmuSTA5fe3HM"
      },
      "source": [
        "df.subject.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXrwjumMe3HV"
      },
      "source": [
        "**MERGING ALL THE TEXT DATA INTO 1 COLUMN i.e. 'text'**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2I8hHsF1e3HW"
      },
      "source": [
        "plt.figure(figsize = (12,8))\n",
        "sns.set(style = \"whitegrid\",font_scale = 1.2)\n",
        "chart = sns.countplot(x = \"subject\", hue = \"category\" , data = df)\n",
        "chart.set_xticklabels(chart.get_xticklabels(),rotation=90)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkraFo09e3Hd"
      },
      "source": [
        "**SINCE THE TOPICS IN SUBJECT COLUMN ARE DIFFERENT FOR BOTH CATEGORIES, HENCE WE HAVE TO EXCLUDE IT FROM FINAL TEXT COLUMN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XZHkgK9e3He"
      },
      "source": [
        "df['text'] = df['text'] + \" \" + df['title']\n",
        "del df['title']\n",
        "del df['subject']\n",
        "del df['date']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKi_ZTaie3Hl"
      },
      "source": [
        "**WHAT ARE STOPWORDS?**\n",
        "\n",
        "**Stopwords are the English words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. For example, the words like the, he, have etc. Such words are already captured this in corpus named corpus. We first download it to our python environment.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9Pguai7e3Hn"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop = set(stopwords.words('english'))\n",
        "punctuation = list(string.punctuation)\n",
        "stop.update(punctuation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ad9Vjcfe3Ht"
      },
      "source": [
        "**DATA CLEANING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8iTlOphe3Hu"
      },
      "source": [
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "#Removing the square brackets\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "# Removing URL's\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub(r'http\\S+', '', text)\n",
        "#Removing the stopwords from text\n",
        "def remove_stopwords(text):\n",
        "    final_text = []\n",
        "    for i in text.split():\n",
        "        if i.strip().lower() not in stop:\n",
        "            final_text.append(i.strip())\n",
        "    return \" \".join(final_text)\n",
        "#Removing the noisy text\n",
        "def denoise_text(text):\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    text = remove_stopwords(text)\n",
        "    return text\n",
        "#Apply function on review column\n",
        "df['text']=df['text'].apply(denoise_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42ne8zslCBag"
      },
      "source": [
        "text_samples = df[:20]\r\n",
        "text_samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0wO6mhCPTn0"
      },
      "source": [
        "df['text'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ab5Goam1DaUW"
      },
      "source": [
        "df.text[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjcJqVuGCVBB"
      },
      "source": [
        "## Split dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjnvZ-rye3I4"
      },
      "source": [
        "**Splitting the data into 2 parts - training and testing data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpAQabt2e3I5"
      },
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(df.text,df.category,random_state = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caANW2ZT7Tw9"
      },
      "source": [
        "samples = x_train[:20]\r\n",
        "samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5Kan-7JEe1p"
      },
      "source": [
        "y_train[:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JP5w9klge3I7"
      },
      "source": [
        "max_features = 10000\n",
        "maxlen = 300"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsJVPArOe3I_"
      },
      "source": [
        "**Tokenizing Text -> Repsesenting each word by a number**\n",
        "\n",
        "**Mapping of orginal word to number is preserved in word_index property of tokenizer**\n",
        "\n",
        "**Tokenized applies basic processing like changing it to lower case, explicitely setting that as False**\n",
        "\n",
        "**Lets keep all news to 300, add padding to news with less than 300 words and truncating long ones**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxETshFke3I_"
      },
      "source": [
        "tokenizer = text.Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "tokenized_train = tokenizer.texts_to_sequences(x_train)\n",
        "x_train = sequence.pad_sequences(tokenized_train, maxlen=maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVpj9iUMA0wi"
      },
      "source": [
        "x_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVhnAh39AqrR"
      },
      "source": [
        "x_train[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ye8N7Ch54Itw"
      },
      "source": [
        "tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0b7dcyY7IGt"
      },
      "source": [
        "### Import tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "li3gG8sh5LeL"
      },
      "source": [
        "import pickle\r\n",
        "\r\n",
        "# saving\r\n",
        "with open('tokenizer.pickle', 'wb') as handle:\r\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTl4gVrN7vhQ"
      },
      "source": [
        "%cd ../"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5Cgn1jf5VVd"
      },
      "source": [
        "import pickle \r\n",
        "\r\n",
        "# loading\r\n",
        "with open('tokenizer.pickle', 'rb') as handle:\r\n",
        "    t = pickle.load(handle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVmXQbYoGvFU"
      },
      "source": [
        "tokenized_samples = t.texts_to_sequences(samples)\r\n",
        "pad_tok_samples = sequence.pad_sequences(tokenized_samples, maxlen=maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGf1aXLT78Si"
      },
      "source": [
        "pad_tok_samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnH3glhG8GGp"
      },
      "source": [
        "tokenized_samples = tokenizer.texts_to_sequences(samples)\r\n",
        "pad_tok_samples = sequence.pad_sequences(tokenized_samples, maxlen=maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Dwc05sB8A58"
      },
      "source": [
        "s = sequence.pad_sequences(tokenized_samples, 300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTVmygeQGVZk"
      },
      "source": [
        "s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUh-f4-6GY-R"
      },
      "source": [
        "pad_tok_samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbGNMggpm2QB"
      },
      "source": [
        "df = pd.DataFrame(x_train)\r\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7J3X4Iv25JAe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1Tx4c8ponhv"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfWXfYEho187"
      },
      "source": [
        "df.values.tolist()[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEh1oz_1pSX4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_Oq6FIuoD9Z"
      },
      "source": [
        "df.to_csv('train_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgBORfb3e3JB"
      },
      "source": [
        "tokenized_test = tokenizer.texts_to_sequences(x_test)\n",
        "X_test = sequence.pad_sequences(tokenized_test, maxlen=maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvlg5ecae3JG"
      },
      "source": [
        "# Introduction to GloVe\n",
        "**GloVe method is built on an important idea,\n",
        "You can derive semantic relationships between words from the co-occurrence matrix.\n",
        "Given a corpus having V words, the co-occurrence matrix X will be a V x V matrix, where the i th row and j th column of X, X_ij denotes how many times word i has co-occurred with word j. An example co-occurrence matrix might look as follows.**\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "**The co-occurrence matrix for the sentence “the cat sat on the mat” with a window size of 1. As you probably noticed it is a symmetric matrix.\n",
        "How do we get a metric that measures semantic similarity between words from this? For that, you will need three words at a time. Let me concretely lay down this statement.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IM0tUBdPe3JG"
      },
      "source": [
        "**![image.png](attachment:image.png)\n",
        "The behavior of P_ik/P_jk for various words\n",
        "Consider the entity\n",
        "P_ik/P_jk where P_ik = X_ik/X_i\n",
        "Here P_ik denotes the probability of seeing word i and k together, which is computed by dividing the number of times i and k appeared together (X_ik) by the total number of times word i appeared in the corpus (X_i).\n",
        "You can see that given two words, i.e. ice and steam, if the third word k (also called the “probe word”),\n",
        "is very similar to ice but irrelevant to steam (e.g. k=solid), P_ik/P_jk will be very high (>1),\n",
        "is very similar to steam but irrelevant to ice (e.g. k=gas), P_ik/P_jk will be very small (<1),\n",
        "is related or unrelated to either words, then P_ik/P_jk will be close to 1\n",
        "So, if we can find a way to incorporate P_ik/P_jk to computing word vectors we will be achieving the goal of using global statistics when learning word vectors.**\n",
        "\n",
        "**Source Credits - https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivzp5-cUpaTt"
      },
      "source": [
        "% cd ../"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwM8b5pTe3JH"
      },
      "source": [
        "EMBEDDING_FILE = 'glove-twitter/glove.twitter.27B.100d.txt'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aB0qgFmnpCTl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kiion4Eue3JM"
      },
      "source": [
        "def get_coefs(word, *arr): \n",
        "    return word, np.asarray(arr, dtype='float32')\n",
        "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0TShM7Be3JP"
      },
      "source": [
        "all_embs = np.stack(embeddings_index.values())\n",
        "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
        "embed_size = all_embs.shape[1]\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "nb_words = min(max_features, len(word_index))\n",
        "#change below line if computing normal stats is too slow\n",
        "embedding_matrix = embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_features: continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8iEWflJe3JS"
      },
      "source": [
        "**Some Model Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhVodwHse3JS"
      },
      "source": [
        "batch_size = 256\n",
        "epochs = 10\n",
        "embed_size = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dDWs4Kye3JU"
      },
      "source": [
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.5, min_lr=0.00001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lk75HL-pe3JX"
      },
      "source": [
        "# TRAINING THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YicqV-psFaS"
      },
      "source": [
        "x_train.astype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bvFzJp4g3kF"
      },
      "source": [
        "### GPU verification\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tF8OQW7g-Ni"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jb_m8jce3JX"
      },
      "source": [
        "#Defining Neural Network\n",
        "model = Sequential()\n",
        "#Non-trainable embeddidng layer\n",
        "model.add(Embedding(max_features, output_dim=embed_size, weights=[embedding_matrix], input_length=maxlen, trainable=False))\n",
        "#LSTM \n",
        "model.add(LSTM(units=128 , return_sequences = True , recurrent_dropout = 0.25 , dropout = 0.25))\n",
        "model.add(LSTM(units=64 , recurrent_dropout = 0.1 , dropout = 0.1))\n",
        "model.add(Dense(units = 32 , activation = 'relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer=keras.optimizers.Adam(lr = 0.01), loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7tzgY-Ce3Ja"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Q4ri2qhe3Jd"
      },
      "source": [
        "history = model.fit(x_train, y_train, batch_size = batch_size , validation_data = (X_test,y_test) , epochs = epochs , callbacks = [learning_rate_reduction])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4q8L54NWhodt"
      },
      "source": [
        "### Save the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKUdzsemhGWN"
      },
      "source": [
        "model.save('model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkwIsznU9a4J"
      },
      "source": [
        "### Load the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QQaBnxatFYj"
      },
      "source": [
        "model = keras.models.load_model('model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYP7yeS09eNy"
      },
      "source": [
        "### Predict samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9w04MxrAFPK"
      },
      "source": [
        "samples.index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaGFVpHf9diJ"
      },
      "source": [
        "model.predict_classes(pad_tok_samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoRw3L2E93hD"
      },
      "source": [
        "y_train[:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwLf7xOEe3Jg"
      },
      "source": [
        "# ANALYSIS AFTER TRAINING OF MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCt_RKVawJ02"
      },
      "source": [
        "X_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYv4qduivLCm"
      },
      "source": [
        "for i in range(10):\r\n",
        "  print(model.predict(X_test[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlWQD0COtcz_"
      },
      "source": [
        "len(model.predict_classes([X_test]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7v3viD1e3Jh"
      },
      "source": [
        "print(\"Accuracy of the model on Training Data is - \" , model.evaluate(x_train[:10],y_train[:10])[1]*100 , \"%\")\n",
        "print(\"Accuracy of the model on Testing Data is - \" , model.evaluate(X_test[:10],y_test[:10])[1]*100 , \"%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4XS1_vxzt4N"
      },
      "source": [
        "y_train[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Owq7I6ZCyC_t"
      },
      "source": [
        "X_test[3].reshape(1,300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6vDlQF2xHot"
      },
      "source": [
        "predictions = model.predict_classes(X_test[:10].reshape(-1,300))\r\n",
        "print(\"predictions shape:\", predictions.shape)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqjTPfeIxMKi"
      },
      "source": [
        "predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpxfOQ_Ie3Jj"
      },
      "source": [
        "epochs = [i for i in range(10)]\n",
        "fig , ax = plt.subplots(1,2)\n",
        "train_acc = history.history['accuracy']\n",
        "train_loss = history.history['loss']\n",
        "val_acc = history.history['val_accuracy']\n",
        "val_loss = history.history['val_loss']\n",
        "fig.set_size_inches(20,10)\n",
        "\n",
        "ax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')\n",
        "ax[0].plot(epochs , val_acc , 'ro-' , label = 'Testing Accuracy')\n",
        "ax[0].set_title('Training & Testing Accuracy')\n",
        "ax[0].legend()\n",
        "ax[0].set_xlabel(\"Epochs\")\n",
        "ax[0].set_ylabel(\"Accuracy\")\n",
        "\n",
        "ax[1].plot(epochs , train_loss , 'go-' , label = 'Training Loss')\n",
        "ax[1].plot(epochs , val_loss , 'ro-' , label = 'Testing Loss')\n",
        "ax[1].set_title('Training & Testing Loss')\n",
        "ax[1].legend()\n",
        "ax[1].set_xlabel(\"Epochs\")\n",
        "ax[1].set_ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xra9G8ce3Jn"
      },
      "source": [
        "pred = model.predict_classes(X_test)\n",
        "pred[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OShxC2nNe3Jq"
      },
      "source": [
        "print(classification_report(y_test, pred, target_names = ['Fake','Not Fake']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhkTJ-27e3Js"
      },
      "source": [
        "cm = confusion_matrix(y_test,pred)\n",
        "cm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGeLFCmQe3J-"
      },
      "source": [
        "cm = pd.DataFrame(cm , index = ['Fake','Original'] , columns = ['Fake','Original'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxgSI3h9e3KC"
      },
      "source": [
        "plt.figure(figsize = (10,10))\n",
        "sns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='' , xticklabels = ['Fake','Original'] , yticklabels = ['Fake','Original'])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWdNo6cXe3KG"
      },
      "source": [
        "**PLS UPVOTE THIS NOTEBOOK IF YOU LIKE IT! THANKS FOR YOUR TIME !**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vO4Y4_vde3KG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}